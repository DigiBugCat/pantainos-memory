# Pantainos Memory - Zettelkasten Knowledge Graph
#
# Authentication: MCP OAuth 2.0 backed by Cloudflare Access
# Configure CF_ACCESS_TEAM and CF_ACCESS_AUD for your Zero Trust application.
#
# Environments:
#   Local:      pnpm dev (uses dev resources with --remote flag)
#   Dev:        wrangler deploy --env dev
#   Production: wrangler deploy

name = "pantainos-memory"
main = "src/index.ts"
compatibility_date = "2024-12-01"
compatibility_flags = ["nodejs_compat"]

# ============================================
# Production Environment (default)
# ============================================

# Scheduled Jobs (Cron Triggers)
[triggers]
crons = ["* * * * *", "0 3 * * *"]

# Workflows
[[workflows]]
name = "exposure-check-workflow"
binding = "EXPOSURE_CHECK"
class_name = "ExposureCheckWorkflow"

[[workflows]]
name = "session-dispatch-workflow"
binding = "SESSION_DISPATCH"
class_name = "SessionDispatchWorkflow"

[[workflows]]
name = "inactivity-cron-workflow"
binding = "INACTIVITY_CRON"
class_name = "InactivityCron"

# D1 Database
[[d1_databases]]
binding = "DB"
database_name = "pantainos-memory-prod"
database_id = "TODO"
migrations_dir = "migrations"

# Vectorize Indexes (768 dimensions, embeddinggemma-300m)
[[vectorize]]
binding = "MEMORY_VECTORS"
index_name = "pantainos-memory-prod-vectors"

[[vectorize]]
binding = "INVALIDATES_VECTORS"
index_name = "pantainos-memory-prod-invalidates"

[[vectorize]]
binding = "CONFIRMS_VECTORS"
index_name = "pantainos-memory-prod-confirms"

# Workers AI
[ai]
binding = "AI"

# Analytics Engine
[[analytics_engine_datasets]]
binding = "ANALYTICS"
dataset = "pantainos_memory_prod"

# OAuth KV (for MCP authentication state)
[[kv_namespaces]]
binding = "OAUTH_KV"
id = "TODO"

# Detection Queue
[[queues.producers]]
queue = "pantainos-memory-prod-detection"
binding = "DETECTION_QUEUE"

[[queues.consumers]]
queue = "pantainos-memory-prod-detection"
max_batch_size = 10
max_batch_timeout = 5

# Configuration Variables
[vars]
REASONING_MODEL = "@cf/openai/gpt-oss-120b"
DEDUP_MODEL = "@cf/openai/gpt-oss-20b"
DEDUP_THRESHOLD = "0.85"
DEDUP_LOWER_THRESHOLD = "0.70"
DEDUP_CONFIDENCE_THRESHOLD = "0.8"
RESOLVER_TYPE = "none"

# ============================================
# Dev Environment
# ============================================
[env.dev]
name = "pantainos-memory-dev"

[env.dev.ai]
binding = "AI"

[env.dev.vars]
REASONING_MODEL = "@cf/openai/gpt-oss-120b"
DEDUP_MODEL = "@cf/openai/gpt-oss-20b"
DEDUP_THRESHOLD = "0.85"
DEDUP_LOWER_THRESHOLD = "0.70"
DEDUP_CONFIDENCE_THRESHOLD = "0.8"
RESOLVER_TYPE = "none"
# Exposure check thresholds (lower = more sensitive)
MIN_SIMILARITY = "0.35"
VIOLATION_CONFIDENCE_THRESHOLD = "0.6"
CONFIRM_CONFIDENCE_THRESHOLD = "0.7"
MAX_CANDIDATES = "30"

[[env.dev.workflows]]
name = "exposure-check-workflow-dev"
binding = "EXPOSURE_CHECK"
class_name = "ExposureCheckWorkflow"

[[env.dev.workflows]]
name = "session-dispatch-workflow-dev"
binding = "SESSION_DISPATCH"
class_name = "SessionDispatchWorkflow"

[[env.dev.workflows]]
name = "inactivity-cron-workflow-dev"
binding = "INACTIVITY_CRON"
class_name = "InactivityCron"

[[env.dev.d1_databases]]
binding = "DB"
database_name = "pantainos-memory-dev"
database_id = "TODO"
migrations_dir = "migrations"

[[env.dev.vectorize]]
binding = "MEMORY_VECTORS"
index_name = "pantainos-memory-dev-vectors"

[[env.dev.vectorize]]
binding = "INVALIDATES_VECTORS"
index_name = "pantainos-memory-dev-invalidates"

[[env.dev.vectorize]]
binding = "CONFIRMS_VECTORS"
index_name = "pantainos-memory-dev-confirms"

[[env.dev.queues.producers]]
queue = "pantainos-memory-dev-detection"
binding = "DETECTION_QUEUE"

[[env.dev.queues.consumers]]
queue = "pantainos-memory-dev-detection"
max_batch_size = 10
max_batch_timeout = 5

[[env.dev.analytics_engine_datasets]]
binding = "ANALYTICS"
dataset = "pantainos_memory_dev"

# OAuth KV (for MCP authentication state)
[[env.dev.kv_namespaces]]
binding = "OAUTH_KV"
id = "TODO"

# Local dev server
[dev]
port = 8794
local_protocol = "http"
