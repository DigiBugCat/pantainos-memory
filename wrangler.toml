# Pantainos Memory - Zettelkasten Knowledge Graph
#
# Authentication: Cloudflare Access (MCP via external FastMCP proxy)
#
# IMPORTANT: Terraform (infra/) is the source of truth for production.
# This file is used for `pnpm dev` (local) and `--env dev` only.
# Production deploys go through `tofu apply` which reads dist/index.js directly.

name = "pantainos-memory"
main = "src/index.ts"
compatibility_date = "2024-12-01"
compatibility_flags = ["nodejs_compat"]

# ============================================
# Production Environment (default)
# ============================================

# Scheduled Jobs (Cron Triggers)
[triggers]
crons = ["* * * * *", "0 3 * * *"]

# D1 Database
[[d1_databases]]
binding = "DB"
database_name = "pantainos-memory"
database_id = "22be27c1-7e29-4fac-81e2-cd96ceadafa2"
migrations_dir = "migrations"

# Vectorize Indexes (768 dimensions, embeddinggemma-300m)
[[vectorize]]
binding = "MEMORY_VECTORS"
index_name = "pantainos-memory-vectors"

[[vectorize]]
binding = "INVALIDATES_VECTORS"
index_name = "pantainos-memory-invalidates"

[[vectorize]]
binding = "CONFIRMS_VECTORS"
index_name = "pantainos-memory-confirms"

# Workers AI
[ai]
binding = "AI"

# Analytics Engine
[[analytics_engine_datasets]]
binding = "ANALYTICS"
dataset = "memory_prod"

# Detection Queue
[[queues.producers]]
queue = "pantainos-memory-detection"
binding = "DETECTION_QUEUE"

[[queues.consumers]]
queue = "pantainos-memory-detection"
max_batch_size = 10
max_batch_timeout = 5
max_retries = 3
dead_letter_queue = "pantainos-memory-detection-dlq"

# Dead Letter Queue for failed commits
[[queues.producers]]
queue = "pantainos-memory-detection-dlq"
binding = "DETECTION_DLQ"

# Configuration Variables
[vars]
REASONING_MODEL = "@cf/openai/gpt-oss-120b"
DEDUP_MODEL = "@cf/openai/gpt-oss-20b"
DEDUP_THRESHOLD = "0.85"
DEDUP_LOWER_THRESHOLD = "0.55"
DEDUP_CONFIDENCE_THRESHOLD = "0.8"
RESOLVER_TYPE = "github"
RESOLVER_GITHUB_REPO = "DigiBugCat/Cassandra-Finance"
CLASSIFICATION_CHALLENGE_ENABLED = "true"
LLM_JUDGE_URL = "https://api.openai.com/v1/chat/completions"
LLM_JUDGE_MODEL = "gpt-5-chat-latest"
EXPOSURE_LLM_MAX_CONCURRENCY = "20"

# ============================================
# Dev Environment
# ============================================
[env.dev]
name = "pantainos-memory-dev"

[env.dev.ai]
binding = "AI"

[env.dev.vars]
REASONING_MODEL = "@cf/openai/gpt-oss-120b"
DEDUP_MODEL = "@cf/openai/gpt-oss-20b"
DEDUP_THRESHOLD = "0.85"
DEDUP_LOWER_THRESHOLD = "0.55"
DEDUP_CONFIDENCE_THRESHOLD = "0.8"
RESOLVER_TYPE = "github"
RESOLVER_GITHUB_REPO = "DigiBugCat/Cassandra-Finance"
CLASSIFICATION_CHALLENGE_ENABLED = "true"
LLM_JUDGE_URL = "https://api.openai.com/v1/chat/completions"
LLM_JUDGE_MODEL = "gpt-5-chat-latest"
# Exposure check thresholds (lower = more sensitive)
MIN_SIMILARITY = "0.35"
VIOLATION_CONFIDENCE_THRESHOLD = "0.6"
CONFIRM_CONFIDENCE_THRESHOLD = "0.7"
MAX_CANDIDATES = "30"
EXPOSURE_LLM_MAX_CONCURRENCY = "20"

[[env.dev.d1_databases]]
binding = "DB"
database_name = "pantainos-memory-dev"
database_id = "cddfba98-685d-4139-933d-1160a00427c2"
migrations_dir = "migrations"

[[env.dev.vectorize]]
binding = "MEMORY_VECTORS"
index_name = "pantainos-memory-dev-vectors"

[[env.dev.vectorize]]
binding = "INVALIDATES_VECTORS"
index_name = "pantainos-memory-dev-invalidates"

[[env.dev.vectorize]]
binding = "CONFIRMS_VECTORS"
index_name = "pantainos-memory-dev-confirms"

[[env.dev.queues.producers]]
queue = "pantainos-memory-dev-detection"
binding = "DETECTION_QUEUE"

[[env.dev.queues.consumers]]
queue = "pantainos-memory-dev-detection"
max_batch_size = 10
max_batch_timeout = 5
max_retries = 3
dead_letter_queue = "pantainos-memory-dev-detection-dlq"

[[env.dev.queues.producers]]
queue = "pantainos-memory-dev-detection-dlq"
binding = "DETECTION_DLQ"

[[env.dev.analytics_engine_datasets]]
binding = "ANALYTICS"
dataset = "memory_dev"

# Local dev server
[dev]
port = 8794
local_protocol = "http"
